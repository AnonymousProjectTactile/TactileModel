

# Active Perception Driven Tactile Modeling of Deformable Objects for Robots




We propose a unified framework
that leverages vision-based tactile sensors to construct fine-
grained object models, enhancing situational understanding.
This framework begins with a multi-task perceptual module
that estimates comprehensive physical properties from tactile
sequences, including contact geometry, force, material hardness,
and dynamic behavior. Secondly, an active exploration module
is developed to optimize sampling efficiency for large objects by
tracking their boundaries and sampling their interiors. Finally,
a visual-tactile registration module is combined to map the
indentation surface from the tactile sensor to a visual scale
with the help of physical attributes, enabling globally consistent
modeling even in deformable scenarios. 


![alt text](assets/pipeline.png)






